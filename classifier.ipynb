{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = 'train/'\n",
    "test = 'test1/'\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "classes = ['dogs', 'cats']\n",
    "n_classes = 2\n",
    "validation = 0.1\n",
    "n_channels = 3\n",
    "img_flat = n_channels * img_size * img_size\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def read_images_in_folder(train_path):\n",
    "    image_stack = []\n",
    "    labels = []\n",
    "    for img in glob.glob('train/*.jpg'): # All jpeg images\n",
    "        label = img[img.find(\"/\")+1:img.find(\".\")]\n",
    "        labels.append(label)\n",
    "        image = cv2.imread(img)\n",
    "        img_resize = cv2.resize(image, (128, 128), cv2.INTER_LINEAR)\n",
    "        image_stack.append(img_resize)\n",
    "    return np.asarray(image_stack), labels\n",
    "data, labels = read_images_in_folder(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [1 if label == 'dog' else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data = loading.read_data(train_path, img_size, classes, validation=validation)\n",
    "#test_images, test_ids = loading.read_data_test(test_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t(25000,)\n",
      "- Test-set:\t\t(25000, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format((labels.shape)))\n",
    "print(\"- Test-set:\\t\\t{}\".format((data.shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  w4 = weight_var(shape=[3, 3, 24, 64])\n",
    "    b4 = bias_var([64])\n",
    "    layer4 = tf.nn.conv2d(layer3, w4, strides=strides1, padding='SAME')\n",
    "    layer4 = tf.nn.bias_add(layer4, b4)\n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    #print(layer3.get_shape)\n",
    "    \n",
    "    w5 = weight_var(shape=[3, 3, 64, 128])\n",
    "    b5 = bias_var([128])\n",
    "    layer5 = tf.nn.conv2d(layer4, w5, strides=strides1, padding='SAME')\n",
    "    layer5 = tf.nn.bias_add(layer5, b5)\n",
    "    layer5 = tf.nn.relu(layer5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_var(shape):\n",
    "    w = tf.truncated_normal(shape=shape, stddev=0.01)\n",
    "    return tf.Variable(w)\n",
    "\n",
    "def bias_var(shape):\n",
    "    b = tf.truncated_normal(shape=shape, stddev=0.01)\n",
    "    return tf.Variable(b)\n",
    "\n",
    "def CNN(x_):\n",
    "    filter_ = 5\n",
    "    strides1 = [1, 1, 1, 1]\n",
    "    strides2 = [1, 2, 2, 1]\n",
    "    ksize = strides2\n",
    "    \n",
    "    w1 = weight_var(shape=[5, 5, 3, 32])   \n",
    "    b1 = bias_var([32])\n",
    "    layer1 = tf.nn.conv2d(x_, w1, strides=strides1, padding='VALID')  #(, 44, 44, 64)\n",
    "    layer1 = tf.nn.bias_add(layer1, b1)\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    #print(layer1.get_shape)\n",
    "    layer1 = tf.nn.max_pool(layer1, ksize=ksize, strides=strides2, padding='VALID') #(, 22, 22, 64)\n",
    "    \n",
    "    \n",
    "    w2 = weight_var(shape=[5, 5, 32, 64])\n",
    "    b2 = bias_var([64])\n",
    "    layer2 = tf.nn.conv2d(layer1, w2, strides=strides1, padding='SAME') #(, 18, 18, 64)\n",
    "    layer2 = tf.nn.bias_add(layer2, b2)\n",
    "    #print(layer2.get_shape)\n",
    "    \n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "     \n",
    "   \n",
    "    layer2 = tf.nn.max_pool(layer2, ksize=ksize, strides=strides2, padding='SAME') #(, 9, 9, 64)\n",
    "    #layer2 = tf.nn.dropout(layer2, 0.5)\n",
    "    \n",
    "    #w3 = weight_var(shape=[5, 5, 18, 36])\n",
    "    #b3 = bias_var([36])\n",
    "    #layer3 = tf.nn.conv2d(layer2, w3, strides=strides1, padding='SAME')\n",
    "    #layer3 = tf.nn.bias_add(layer3, b3)\n",
    "    #layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "    \n",
    "    layer5= tf.nn.max_pool(layer2, ksize=ksize, strides=strides2, padding='SAME')\n",
    "    \n",
    "    #layer5 = tf.nn.dropout(layer5, 0.5)\n",
    "    \n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(layer5)\n",
    "    dim = flatten.get_shape().as_list()\n",
    "    print(dim[1])\n",
    "    \n",
    "    wc1 = weight_var(shape=[(dim[1]), 128])\n",
    "    b_c1 = bias_var([128])\n",
    "    fc1 = tf.add(tf.matmul(flatten, wc1), b_c1)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    wc2 = weight_var(shape=[128, 2])\n",
    "    b_c2 = bias_var([2])\n",
    "    logits = tf.add(tf.matmul(fc1, wc2), b_c2)\n",
    "    #fc2 = tf.nn.relu(fc2)\n",
    "    #fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val,Y_tr, Y_val = train_test_split(data, labels, test_size = 0.1, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "n_labels = len(set(labels))\n",
    "feat = data.shape[1]\n",
    "channels = data.shape[3]\n",
    "batch_size = 32\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, feat, feat, channels))\n",
    "#x_reshape = tf.reshape(x,[-1, img_size, img_size, n_channels] )\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(tf.int32, shape=(None))\n",
    "y_hot = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n"
     ]
    }
   ],
   "source": [
    "logits = CNN(x)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(y_hot, 1), tf.argmax(softmax, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats(X_batch, Y_batch):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    samples = len(X_batch)\n",
    "    #X_batch = np.reshape(X_batch, [-1, n_steps, n_inputs])\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, samples, batch_size):\n",
    "        end = offset+batch_size\n",
    "        xs_batch, ys_batch = X_batch[offset:end], Y_batch[offset:end]\n",
    "        loss_, acc = sess.run([loss, accuracy], feed_dict={x:xs_batch, \n",
    "                                                           y:ys_batch,\n",
    "                                                           keep_prob:1.0\n",
    "                                                           \n",
    "                                                           })\n",
    "        total_loss += (float(loss_)*xs_batch.shape[0])\n",
    "        total_acc += (float(acc)*xs_batch.shape[0])\n",
    "\n",
    "    return (total_loss)/samples, (total_acc)/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 128, 128, 3) (22500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, Y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_map = []\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "samples = len(X_tr)\n",
    "\n",
    "count = 0\n",
    "print(\"Initializing Training\")\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            X_tr, Y_tr = shuffle(X_tr, Y_tr)\n",
    "            for offset in range(0, samples, batch_size):\n",
    "                end = offset+batch_size\n",
    "                xs_batch, ys_batch = X_tr[offset:end], Y_tr[offset:end]\n",
    "                loss_, _ = sess.run([loss, optimizer], feed_dict={x: xs_batch, \n",
    "                                                                  y: ys_batch,\n",
    "                                                                  keep_prob:0.5})    \n",
    "\n",
    "            validation_loss, validation_acc = stats(X_val, Y_val)\n",
    "            acc_map.append(round(validation_acc*100, 2))\n",
    "            print(\"EPOCH {}\".format(epoch + 1))\n",
    "            print (\"Validation Loss = {:.3f} and Validation Accuracy = {:.3f} %\".format(validation_loss, 100 *validation_acc))\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.xticks(range(1, epochs+1))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.grid(True)\n",
    "        plt.plot(range(1, epochs+1), acc_map, '-o')\n",
    "        for i, j in list(zip(range(1, epochs+1), acc_map))[0::3]:\n",
    "            plt.text(i, j-1, str(j))\n",
    "        #test_loss, test_acc = sess.run([loss, accuracy], feed_dict={x: xs_batch, \n",
    "                                                                    #y: ys_batch,\n",
    "                                                                    #keep_prob:1.0})  \n",
    "        print (\"Test Loss {} and Test Accuracy {:.3f} %\".format(test_loss, 100 *test_acc))\n",
    "except KeyboardInterrupt:\n",
    "    print('Training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
